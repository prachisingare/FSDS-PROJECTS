# 🤖 HF Bootcamp - Hugging Face Transformers

Welcome to the **HF Bootcamp**! This project explores Hugging Face’s powerful tools and models through a guided notebook. It is designed as a hands-on introduction to Natural Language Processing (NLP), using pre-trained transformers for various text-based tasks.

---

## 📚 Project Description

The goal of this bootcamp is to:
- Understand the Hugging Face ecosystem
- Learn how to use transformer-based models like BERT or DistilBERT
- Perform tokenization and model inference
- Load and preprocess public datasets from Hugging Face
- Train and evaluate text classification models

---

## 🛠️ Technologies Used

| Tool/Library     | Purpose                            |
|------------------|------------------------------------|
| Python 🐍         | Programming Language               |
| Hugging Face 🤗   | NLP Transformers and Datasets     |
| PyTorch 🔥        | Deep Learning Framework           |
| Google Colab 📓    | Notebook Execution Environment    |
| Matplotlib 📊      | Data Visualization                |
| Pandas 📋          | Data Handling                     |

---


## 💡 Learnings
Hands-on with transformer-based architectures

Fine-tuning pre-trained models for text classification

Use of Hugging Face’s simple and powerful APIs

Integration of datasets, tokenizers, and metrics into one pipeline

---

🔗 Useful Links

## 🧠 Concepts Covered
- NLP pipeline using Hugging Face

- Dataset loading and formatting

- Tokenizer usage

- Transformer model loading and configuration

- Training loop with metrics tracking

- Inference on new text samples



## ✍️ Author
Prachi Singare
- 🎓 BTech in Artificial Intelligence and Data Science
- 💻 Passionate about NLP, Machine Learning, and Data Science

# ğŸ¤– HF Bootcamp - Hugging Face Transformers

Welcome to the **HF Bootcamp**! This project explores Hugging Faceâ€™s powerful tools and models through a guided notebook. It is designed as a hands-on introduction to Natural Language Processing (NLP), using pre-trained transformers for various text-based tasks.

---

## ğŸ“š Project Description

The goal of this bootcamp is to:
- Understand the Hugging Face ecosystem
- Learn how to use transformer-based models like BERT or DistilBERT
- Perform tokenization and model inference
- Load and preprocess public datasets from Hugging Face
- Train and evaluate text classification models

---

## ğŸ› ï¸ Technologies Used

| Tool/Library     | Purpose                            |
|------------------|------------------------------------|
| Python ğŸ         | Programming Language               |
| Hugging Face ğŸ¤—   | NLP Transformers and Datasets     |
| PyTorch ğŸ”¥        | Deep Learning Framework           |
| Google Colab ğŸ““    | Notebook Execution Environment    |
| Matplotlib ğŸ“Š      | Data Visualization                |
| Pandas ğŸ“‹          | Data Handling                     |

---


## ğŸ’¡ Learnings
Hands-on with transformer-based architectures

Fine-tuning pre-trained models for text classification

Use of Hugging Faceâ€™s simple and powerful APIs

Integration of datasets, tokenizers, and metrics into one pipeline

---

ğŸ”— Useful Links

## ğŸ§  Concepts Covered
- NLP pipeline using Hugging Face

- Dataset loading and formatting

- Tokenizer usage

- Transformer model loading and configuration

- Training loop with metrics tracking

- Inference on new text samples



## âœï¸ Author
Prachi Singare
- ğŸ“ BTech in Artificial Intelligence and Data Science
- ğŸ’» Passionate about NLP, Machine Learning, and Data Science
